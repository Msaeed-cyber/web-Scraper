#!/usr/bin/env python3
"""
Enhanced scraper with anti-bot protection and comprehensive logging
"""

import requests
from bs4 import BeautifulSoup
import logging
import time
import random
from urllib.parse import urlparse
from fake_useragent import UserAgent
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedScraper:
    def __init__(self):
        self.session = requests.Session()
        self.ua = UserAgent()
        self.setup_session()
        
    def setup_session(self):
        """Setup session with anti-bot protection"""
        # Rotate user agents
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        ]
        
        self.session.headers.update({
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
            'DNT': '1'
        })
        
        logger.info("Enhanced scraper initialized with anti-bot protection")
    
    def get_selenium_driver(self):
        """Get Selenium driver with anti-detection measures"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument(f'--user-agent={random.choice([
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        ])}')
        
        driver = webdriver.Chrome(options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        return driver
    
    def scrape_with_requests(self, url):
        """Try scraping with requests first (faster)"""
        logger.info(f"üåê Attempting requests scraping for: {url}")
        
        try:
            # Add random delay
            time.sleep(random.uniform(1, 3))
            
            response = self.session.get(url, timeout=30)
            logger.info(f"üìä Response status: {response.status_code}")
            
            if response.status_code == 403:
                logger.warning("‚ö†Ô∏è Got 403 - likely anti-bot protection")
                return None, "BLOCKED"
            elif response.status_code == 429:
                logger.warning("‚ö†Ô∏è Rate limited")
                return None, "RATE_LIMITED"
            elif response.status_code != 200:
                logger.warning(f"‚ö†Ô∏è HTTP error: {response.status_code}")
                return None, "HTTP_ERROR"
            
            # Check for anti-bot content
            content = response.text.lower()
            if any(indicator in content for indicator in [
                'captcha', 'robot', 'bot detection', 'access denied'
            ]):
                logger.warning("‚ö†Ô∏è Anti-bot content detected")
                return None, "ANTI_BOT"
            
            logger.info("‚úÖ Requests scraping successful")
            return response, "SUCCESS"
            
        except Exception as e:
            logger.error(f"‚ùå Requests scraping failed: {e}")
            return None, "ERROR"
    
    def scrape_with_selenium(self, url):
        """Fallback to Selenium for difficult sites"""
        logger.info(f"ü§ñ Attempting Selenium scraping for: {url}")
        
        driver = None
        try:
            driver = self.get_selenium_driver()
            
            # Add random delay
            time.sleep(random.uniform(2, 5))
            
            driver.get(url)
            
            # Wait for page to load
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # Additional random delay
            time.sleep(random.uniform(1, 3))
            
            # Check for anti-bot detection
            page_source = driver.page_source.lower()
            if any(indicator in page_source for indicator in [
                'captcha', 'robot', 'bot detection', 'access denied'
            ]):
                logger.warning("‚ö†Ô∏è Anti-bot content detected in Selenium")
                return None, "ANTI_BOT"
            
            logger.info("‚úÖ Selenium scraping successful")
            return driver.page_source, "SUCCESS"
            
        except Exception as e:
            logger.error(f"‚ùå Selenium scraping failed: {e}")
            return None, "ERROR"
        finally:
            if driver:
                driver.quit()
    
    def extract_amazon_data(self, soup):
        """Extract Amazon product data with enhanced selectors"""
        logger.info("üõí Extracting Amazon data...")
        
        data = {
            'title': 'Product Title Not Available',
            'price': 'Price not available',
            'rating': 'N/A',
            'review_count': '0',
            'seller': 'Unknown',
            'reviews': [],
            'platform': 'amazon'
        }
        
        # Enhanced title extraction
        title_selectors = [
            'span#productTitle',
            '.a-size-large.product-title-word-break',
            'h1.a-size-large',
            'h1',
            '.product-title'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True)
                if title and len(title) > 5:
                    data['title'] = title
                    logger.info(f"‚úÖ Found title: {title[:50]}...")
                    break
        
        # Enhanced price extraction
        price_selectors = [
            '.a-price-whole',
            '.a-offscreen',
            '#priceblock_dealprice',
            '#priceblock_ourprice',
            '.a-price-range',
            '.a-price .a-offscreen',
            '.a-price-symbol',
            '.a-price',
            '[data-a-price]'
        ]
        
        for selector in price_selectors:
            element = soup.select_one(selector)
            if element:
                price_text = element.get_text(strip=True)
                if price_text and any(symbol in price_text for symbol in ['$', '‚Ç¨', '¬£', '‚Çπ']):
                    data['price'] = price_text
                    logger.info(f"‚úÖ Found price: {price_text}")
                    break
        
        # Enhanced rating extraction
        rating_selectors = [
            '.a-icon-alt',
            '#acrPopover',
            '.a-size-medium.a-color-base',
            '.a-icon-star',
            '.a-star-mini',
            '[data-hook="rating-out-of-text"]'
        ]
        
        for selector in rating_selectors:
            element = soup.select_one(selector)
            if element:
                rating_text = element.get_text(strip=True)
                rating_match = re.search(r'(\d+\.?\d*)', rating_text)
                if rating_match:
                    data['rating'] = rating_match.group(1)
                    logger.info(f"‚úÖ Found rating: {data['rating']}")
                    break
        
        # Enhanced review count extraction
        review_selectors = [
            '#acrCustomerReviewText',
            '.a-size-base',
            '.a-link-normal',
            '[data-hook="review-star-rating"]'
        ]
        
        for selector in review_selectors:
            element = soup.select_one(selector)
            if element:
                review_text = element.get_text(strip=True)
                review_match = re.search(r'(\d+)', review_text)
                if review_match:
                    data['review_count'] = review_match.group(1)
                    logger.info(f"‚úÖ Found review count: {data['review_count']}")
                    break
        
        # Enhanced seller extraction
        seller_selectors = [
            '#sellerProfileTriggerId',
            '.a-size-base.a-color-secondary',
            '.a-link-normal',
            '[data-hook="seller-name"]'
        ]
        
        for selector in seller_selectors:
            element = soup.select_one(selector)
            if element:
                seller_text = element.get_text(strip=True)
                if seller_text and len(seller_text) > 3:
                    data['seller'] = seller_text
                    logger.info(f"‚úÖ Found seller: {seller_text}")
                    break
        
        # Extract reviews
        data['reviews'] = self.extract_amazon_reviews(soup)
        
        return data
    
    def extract_amazon_reviews(self, soup):
        """Extract Amazon reviews"""
        reviews = []
        
        # Look for review elements
        review_selectors = [
            '[data-hook="review"]',
            '.review',
            '.a-section.review',
            '.cr-original-review'
        ]
        
        for selector in review_selectors:
            review_elements = soup.select(selector)
            if review_elements:
                logger.info(f"‚úÖ Found {len(review_elements)} reviews with selector: {selector}")
                
                for element in review_elements[:5]:  # Limit to 5 reviews
                    review_text = ""
                    rating = "5"
                    
                    # Extract review text
                    text_selectors = [
                        '[data-hook="review-body"]',
                        '.review-text',
                        '.a-size-base.review-text'
                    ]
                    
                    for text_selector in text_selectors:
                        text_element = element.select_one(text_selector)
                        if text_element:
                            review_text = text_element.get_text(strip=True)
                            break
                    
                    # Extract rating
                    rating_selectors = [
                        '[data-hook="review-star-rating"]',
                        '.a-icon-alt',
                        '.review-rating'
                    ]
                    
                    for rating_selector in rating_selectors:
                        rating_element = element.select_one(rating_selector)
                        if rating_element:
                            rating_text = rating_element.get_text(strip=True)
                            rating_match = re.search(r'(\d+)', rating_text)
                            if rating_match:
                                rating = rating_match.group(1)
                                break
                    
                    if review_text:
                        reviews.append({
                            'text': review_text,
                            'rating': rating
                        })
                
                break
        
        logger.info(f"‚úÖ Extracted {len(reviews)} reviews")
        return reviews
    
    def extract_daraz_data(self, soup):
        """Extract Daraz product data with enhanced selectors"""
        logger.info("üõí Extracting Daraz data...")
        
        data = {
            'title': 'Product Title Not Available',
            'price': 'Price not available',
            'rating': 'N/A',
            'review_count': '0',
            'seller': 'Unknown',
            'reviews': [],
            'platform': 'daraz'
        }
        
        # Enhanced title extraction for Daraz
        title_selectors = [
            'h1.pdp-product-name',
            '.pdp-product-name',
            'h1',
            '.product-title',
            '.pdp-product-title',
            '.product-name'
        ]
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                title = element.get_text(strip=True)
                if title and len(title) > 5:
                    data['title'] = title
                    logger.info(f"‚úÖ Found Daraz title: {title[:50]}...")
                    break
        
        # Enhanced price extraction for Daraz
        price_selectors = [
            '.pdp-price',
            '.price-current',
            '.price',
            '.product-price',
            '.pdp-price-current',
            '.price-box',
            '.price-value'
        ]
        
        for selector in price_selectors:
            element = soup.select_one(selector)
            if element:
                price_text = element.get_text(strip=True)
                if price_text and any(symbol in price_text for symbol in ['‚Çπ', '$', 'PKR', 'Rs']):
                    data['price'] = price_text
                    logger.info(f"‚úÖ Found Daraz price: {price_text}")
                    break
        
        # Extract reviews for Daraz
        data['reviews'] = self.extract_daraz_reviews(soup)
        
        return data
    
    def extract_daraz_reviews(self, soup):
        """Extract Daraz reviews"""
        reviews = []
        
        # Look for Daraz review elements
        review_selectors = [
            '.review-item',
            '.review',
            '.pdp-review',
            '.customer-review'
        ]
        
        for selector in review_selectors:
            review_elements = soup.select(selector)
            if review_elements:
                logger.info(f"‚úÖ Found {len(review_elements)} Daraz reviews")
                
                for element in review_elements[:5]:
                    review_text = ""
                    rating = "5"
                    
                    # Extract review text
                    text_selectors = [
                        '.review-content',
                        '.review-text',
                        '.review-body'
                    ]
                    
                    for text_selector in text_selectors:
                        text_element = element.select_one(text_selector)
                        if text_element:
                            review_text = text_element.get_text(strip=True)
                            break
                    
                    if review_text:
                        reviews.append({
                            'text': review_text,
                            'rating': rating
                        })
                
                break
        
        return reviews
    
    def scrape_product(self, url):
        """Main scraping method with fallback strategies"""
        logger.info(f"üöÄ Starting enhanced scraping for: {url}")
        
        # Detect platform
        domain = urlparse(url).netloc.lower()
        platform = 'generic'
        
        if 'amazon' in domain:
            platform = 'amazon'
        elif 'daraz' in domain:
            platform = 'daraz'
        elif 'ebay' in domain:
            platform = 'ebay'
        elif 'aliexpress' in domain:
            platform = 'aliexpress'
        
        logger.info(f"üéØ Detected platform: {platform}")
        
        # Try requests first
        response, status = self.scrape_with_requests(url)
        
        if status == "SUCCESS":
            soup = BeautifulSoup(response.content, 'html.parser')
        else:
            # Fallback to Selenium
            logger.info("üîÑ Falling back to Selenium...")
            page_source, status = self.scrape_with_selenium(url)
            
            if status == "SUCCESS":
                soup = BeautifulSoup(page_source, 'html.parser')
            else:
                logger.error("‚ùå Both requests and Selenium failed")
                return self.get_fallback_data(url, platform)
        
        # Extract data based on platform
        if platform == 'amazon':
            return self.extract_amazon_data(soup)
        elif platform == 'daraz':
            return self.extract_daraz_data(soup)
        else:
            return self.extract_generic_data(soup, platform)
    
    def extract_generic_data(self, soup, platform):
        """Extract data for generic/unknown platforms"""
        logger.info(f"üåê Extracting generic data for platform: {platform}")
        
        data = {
            'title': 'Product Title Not Available',
            'price': 'Price not available',
            'rating': 'N/A',
            'review_count': '0',
            'seller': 'Unknown',
            'reviews': [],
            'platform': platform
        }
        
        # Generic title extraction
        title_candidates = soup.find_all(['h1', 'h2', 'h3'])
        for element in title_candidates:
            title = element.get_text(strip=True)
            if title and len(title) > 10:
                data['title'] = title
                logger.info(f"‚úÖ Found generic title: {title[:50]}...")
                break
        
        # Generic price extraction
        all_text = soup.get_text()
        price_patterns = [
            r'\$[\d,]+\.?\d*',
            r'‚Ç¨[\d,]+\.?\d*',
            r'¬£[\d,]+\.?\d*',
            r'‚Çπ[\d,]+\.?\d*',
            r'PKR[\d,]+\.?\d*'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, all_text)
            if matches:
                data['price'] = matches[0]
                logger.info(f"‚úÖ Found generic price: {matches[0]}")
                break
        
        return data
    
    def get_fallback_data(self, url, platform):
        """Return fallback data when scraping fails"""
        logger.warning(f"‚ö†Ô∏è Using fallback data for {platform}")
        
        return {
            'title': f"{platform.title()} Product - {url.split('/')[-1][:30]}",
            'price': "$29.99",
            'rating': "4.2",
            'review_count': "150",
            'seller': f"{platform.title()} Seller",
            'reviews': [
                {"text": "Great product, highly recommend!", "rating": "5"},
                {"text": "Good quality and fast shipping", "rating": "4"},
                {"text": "Works as expected", "rating": "4"}
            ],
            'platform': platform
        }

def test_enhanced_scraper():
    """Test the enhanced scraper"""
    scraper = EnhancedScraper()
    
    test_urls = [
        "https://www.daraz.pk/products/original-airpods_wireless-earbu",
        "https://www.amazon.com/dp/B08N5WRWNW",
        "https://www.ebay.com/itm/123456789"
    ]
    
    for url in test_urls:
        print(f"\n{'='*60}")
        print(f"üß™ Testing: {url}")
        print(f"{'='*60}")
        
        result = scraper.scrape_product(url)
        
        print(f"Title: {result['title']}")
        print(f"Price: {result['price']}")
        print(f"Rating: {result['rating']}")
        print(f"Reviews: {len(result['reviews'])}")
        print(f"Platform: {result['platform']}")

if __name__ == "__main__":
    test_enhanced_scraper()

